# A new design for Boost

> This is a technical overview of the new architecture for Boost which we launched in early 2023. The main goal of this document is serving as an accurate technical description to be shared with our Patent Lawyers as part of finalizing our current patent process. It is also intended as an on-boarding tool for engineers who need to hack on Boost-related parts of Vitess. **It is not meant for public consumption**.

### In the beginning, there was Noria

The first beta version of Boost, which we soft-launched in November 2022, was based on the same architecture as Noria. Noria is an experimental OSS database designed by Jon Gjengset as part of his [PhD Thesis](https://jon.thesquareplanet.com/papers/phd-thesis.pdf). The code for Noria is [Open Source on GitHub](https://github.com/mit-pdos/noria), and it was initially presented in [a technical paper](https://jon.tsp.io/papers/osdi18-noria.pdf) at [OSDI'18](https://www.usenix.org/conference/osdi18/presentation/gjengset).

The key-point in Noria's design is that **it is a standalone database system**. This is in contrast to Boost, which is _not_ a standalone database system but a supplementary distributed system that acts as a caching layer on top of an existing database – in this case, a full Vitess cluster.

As a standalone database, Noria has a data storage layer. The actual data inserted into a Noria system is stored in a key-value database powered by [RocksDB](https://rocksdb.org/), with each table occupying a RocksDB namespace. That is the extent of the data storage capabilities that the Noria system possesses: it can insert data into these tables, and query data by its primary key. It also keeps secondary indexes for when the data needs to be queried by another column. 

In order to actually fulfill its purpose as a SQL-compatible database, Noria needs to transform the individual rows stored in the tables into the results of the SQL query that the user has requested. This is the core of Noria’s paper and PhD thesis. Noria is not a database that can be queried by arbitrary SQL queries. Instead, Noria needs to know ahead of time the exact queries that the user will be performing on the database, and based on this information, it creates a dataflow graph: a tree where the roots are the tables where the row data is stored (henceforth known as “bases”) and where the leaves are either fully or partially materialized views that represent each user’s specific SQL query (henceforth known as “readers”).

Between the Bases and the Readers, the dataflow graph contains a series of SQL operators that perform the actual transformations requested by the user (things like aggregating rows, transforming the values of columns, joining them, etc). If we consider this system where all the Readers are fully materialized views (i.e. where each Reader contains all the rows that could possibly be requested by a user’s query), the design of Noria is very similar to a traditional materialization engine, and not really novel. The fully materialized view acts as a fully populated table that can be queried by one or more constraints that exist in the user’s original query. For instance, a query like `SELECT table.a, SUM(table.b) FROM table WHERE table.a = ?` would result in a table keyed by `table.a` where each value contains the pre-computed value for `table.a, SUM(table.b)` for the given value of `table.a`.  In order to generate this table, which is capable of efficiently serving the user’s specific SQL query by converting it into a key-lookup, we need to do two things:

1. When the Noria system boots up, it needs to iterate through all the individual rows in the Bases, and pass them through all the SQL operators in the flow graph for this specific query, so the relevant transformations are applied (i.e. the `SUM` in this example) and the fully materialized view can be populated.
2. While the Noria system is running, every time a user inserts, deletes or updates data into the Bases, the new data needs to be passed through all the SQL operators in the flow graph, so that the fully materialized view can be kept up to date.

This is, again, very similar to a traditional materialization engine, and not particularly interesting.

The interesting part is when we consider the Readers as _partially_ materialized views: this is the key novel concept in Gjengset’s PhD thesis. A partially materialized view behaves like a fully materialized view, but it contains _gaps_ or _missing keys_. Its table is not fully populated. The behavior of this table is very different:

1. When the Noria system boots up, it does _not_ need to iterate through all the existing rows in the Bases. The final partially materialized view can start as an empty table.
2. While the Noria system is running, every time a user modifies data, it still needs this modification to be passed through the flow graph – although most of the time the information about these modifications will be discarded.

The main reason to have a _partially_ materialized view is to minimize the amount of data to be processed and the memory that the view will occupy in our system, as to not actively hold and maintain up-to-date rows that will be rarely read by the user. The partially materialized view always starts up being _empty_, and it only acquires data _lazily_, when the user requests it.

The process of data acquisition when a user _misses_ during a read to a partial view is one of the most complex technical details in the design of Noria. Let’s dive into it.

### Misses in Noria

We have seen how Noria keeps materialized state in several of the operators in its dataflow graph: on-disk state on its Bases (persisted with RocksDB) and in-memory state on its Readers. However, these are not the only places where state is kept. Noria also keeps memory tables used to calculate the output of many of the possible SQL operators in the dataflow. For instance, aggregation operators need to materialize the individual counts of the aggregation so they can be updated incrementally.

These intermediate materializations must be taken into account when _resolving_ misses in the system. When a request for a specific key arrives and the key is missing in the memory table for the Reader, the system attempts to resolve the miss _recursively_ by asking **the first ancestor of the Reader in the dataflow graph that has materialized state**. The operation of asking an ancestor to fill a miss is called an **upquery** throughout the Noria paper and dissertation. If this ancestor contains the desired data, the miss is resolved by triggering **a replay**. A replay means passing these rows through the dataflow graph all the way to the reader, so that they result in filling the target key on which we missed. If this ancestor with materialized state does not contain the data we require, then we recurse to the next ancestor with materialized state and perform the upquery there. This recursion resolves itself elegantly: if the second-order ancestor (this is, the ancestor of the ancestor) has the data, we trigger a replay which fills the miss in the first ancestor, and then this ancestor triggers its own replay, finally filling the miss on the Reader. Often, despite the intermediate state, a Miss on a Reader must fall back all the way to a Base. The recursive algorithm will always stop at the Base, because a Base is always _fully materialized_. It cannot miss.

This recursive algorithm is elegant, but not efficient. Although in some particular SQL dataflow graphs a replay is a quite efficient operation, most often it becomes a pathological performance issue. Picture a simple dataflow where we’re trying to fill a miss for the `COUNT(*)` of all the rows filtered by a specific column (`SELECT COUNT(*) from tbl where tbl.a = ?`). When we miss for any of the values of `tbl.a`, we trigger an upquery to the immediate ancestor, which here is the aggregation operator. Assuming the operator also misses, we need to fall back to the Base for `tbl`. The Base will not miss, but it will need to replay what we previously described as “required data” – in this case, all the rows in `tbl` where `tbl.a` equals our miss key. This could be millions, or even billions of rows! And they all need to be replayed, one by one, into the aggregation operator which will simply count them and store their count.

This extremely common case, which applies to any SQL query that performs an aggregation or grouping, can also be generalized to more SQL operators. The underlying issue is that Noria, as a stand-alone database system, does not have support for _directly executing SQL queries_. All it can do is feed individual rows through the SQL operators in its dataflow to compute the final results, but in many cases these calculations require millions of rows as an input. In practical effects, this means that filling misses in Noria is often tens or hundreds of times slower than just directly calculating the query in a traditional relational database.

### The first version of Boost

For the first prototype of Boost, and all the way to its first beta release, we followed Noria’s architecture very closely. Although Boost is implemented as a Go service in the Vitess codebase, we ported over a lot of the abstractions and designs in Noria’s Rust code. There were of course significant changes to be done, because after all Boost was implemented as a _cache_ on top of a Vitess cluster, while Noria was a stand-alone database.

The first major change was replacing the concept of a Base from Noria with an External Base. As a cache, Boost didn’t keep its own data. It had the underlying MySQL shards of the Vitess cluster as an authoritative source of truth. Hence, there was no need for a Base that stored individual rows. The roots of the dataflow graph in a Boost instance are an abstraction called External Base, which behave very similarly to a Base in Noria. Namely, they can be upqueried for the required data during misses, and they can replay this data into the Boost cluster; instead of replaying from disk like a Base in Noria, they replay over the network from the relevant MySQL instances. We call the process of performing an upquery on an External Base an **external upquery**. These external upqueries are handled by a version of Vitess’ client routing code, similarly to how a VTGate would serve an arbitrary SQL query.

The second major change is a consequence of removing Bases. Since the system doesn’t contain an authoritative source of the data like in Noria, we need a way to keep the system up to date. In the original Noria, when a write (insert, update or delete) came into the system, it would be applied directly to the base, and then forwarded through the dataflow operators to keep them updated. Writes into a Vitess cluster do not go through Boost —only reads do—, so to stay updated we have to subscribe to the VStream of the cluster (this is, an aggregation of the binary logs of all the MySQL instances in the cluster). This subscription allows us to witness changes to the underlying data, with some latency, and to pass them through the dataflow to keep our cached data up to date, but they introduce a complex data race.

As we’ve just explained, when a there’s a miss in a Boost Reader, we apply the same algorithm as Noria did — a recursive algorithm that will often fall back all the way to the base in order to trigger a replay of the missing data. In our case, the Base is not a local on-disk table but the existing Vitess cluster, so the replay will be the output of a specifically crafted SQL query to fetch the relevant rows from the MySQL instances. This external upquery, which can be triggered at any point as part of resolving a miss, is inherently racy with the changes we receive from the VStream. There can be rows in the query which are going to fill the miss and which we’ve never seen before, because they’ve just been inserted or updated in the underlying cluster. Since these are new rows, they could also be arriving at the exact same moment through the VStream subscription. Hence, the dataflow operators would process the same rows twice, leading to incorrect or inconsistent results.

To solve this problem, we developed an algorithm that is applied on each external base of the system and which uses GTID tracking to prioritize or mark rows as duplicate from the External Base queries and the VStream as to ensure there are rows processed twice through the dataflow. **This is the main subject of our preliminary patent filing for Boost**.

These two modifications are the main differentiators between Noria and Boost, and this is the design we launched as a limited beta on late November. However, after actual production testing of the system, we uncovered two glaring issues which forced us to re-architect Boost:

1. The underlying performance issue in Noria’s design, caused by having to replay individual rows from the Bases, is exacerbated by replacing the Bases with an external source of data.
2. The GTID tracker is not logically correct in all circumstances, and hence our preliminary patent filing is not particularly useful. The system is still vulnerable to seeing duplicated data in some dataflow layouts, which we will discuss later.

### The terrible performance of misses

We had suspicions that the way Noria handled misses would also affect Boost, as the design for filling misses was essentially identical, but it turned out that the problem was much worse. Replaying individual rows over the network was an order of magnitude more expensive than replaying them from local storage, mostly because of latency and (de)serialization costs. In practice, the performance when filling misses in Boost was so bad that most Boost queries that missed would simply time out.

The reason why the miss-filling algorithm in Noria is recursive is because it cannot compute SQL expressions directly. Upqueries would always end up missing throughout the dataflow all the way to a Base, because when you upquery an intermediate operator in Noria and the data is missing, the only possible way to fill this data is by replaying every individual row that is required to compute the data through the operator. This is incredibly inefficient.

The new algorithm exploits the fact that our underlying data store, a Vitess cluster backed by several MySQL instances, can actually compute SQL expressions without needing to replay data. It works as follows:

1. When a query misses on a Reader, instead of falling back to the first immediate parent with a partial materialization, we fall back to the _last one_. That is, we target the first partially materialized node in the dataflow when walking from the External Base.
2. This node, just like all the other nodes with state in the graph, has been extended with an _external upquery_ for the node, calculated at plan time. This means that our planner, after decomposing all the SQL primitives in the original query into individual operators for the dataflow, also extends each operator with its reverse-equivalent SQL expression.
3. To fill the miss on the target node, we perform the reverse-equivalent SQL expression for the operator in our upstream database. We use Vitess’ internal routing logic (the same logic that can be found inside a VTGate) to route the complex SQL across shards.
4. The results of this query are **not** considered a replay, as they do not contain _individual rows_ to be forwarded through the dataflow operator. Instead, these results are specifically tagged so that they fill the state in the target node by being _inserted directly_ into its state table, not processed (note that the “processing” that would be performed on the rows of a replay has already been performed by MySQL here).
5. If necessary, once the state has been filled, a replay is triggered to forward the new rows downstream all the way to the Reader.

To highlight the importance of point 4, let us see the difference between the way Noria and Boost would fill a miss on our previous example query (`SELECT COUNT(*) FROM tbl WHERE tbl.a = ?`). In an original Noria replay, to fill the miss for value `X`, the aggregation operator node would receive a replay containing all the rows in `tbl` where `tbl.a = X`. Then, it would process each row individually, accumulate their count, and store their count `C` in its state table. The result `C` is the single-row, single-column output of the operator, which will then be forwarded through any downstream operators in the dataflow until the miss is filled in the Reader. Hence, the aggregation operator has processed an arbitrary amount of rows, $O(n)$ with the total number of rows in the underlying table.

In contrast, the new algorithm in Boost would fill the miss by acquiring the reverse-equivalent SQL expression for the operator and extending it with a filter by `X`. In this particular example, the resulting query would be something almost identical to the user’s original query (e.g. `SELECT COUNT(*) FROM tbl WHERE tbl.a = X`), because the dataflow for this query is trivial, but for more complex and nested queries, the reverse-equivalent expression for an operator is usually a subset of the user’s original query. The composed query is then performed directly on the Vitess cluster, and the result `C` is _inserted directly_ into the state table for the operator, and forwarded downstream until it reaches the Reader. Note that the operator has not actually _processed_ the result of the external upquery — the processing has been performed by MySQL, which is why we receive a single row as the result, as opposed to $N$ individual rows that must be counted in Boost.

Lastly, also note that the design decision in point 1 is not mandatory: these external upqueries have been implemented on the nearest materialized node _from the External Base_ as a simplification. They could be performed at any height in the dataflow graph, but if the upqueried operator is not directly reachable from the External Base, all the other partially materialized operators above it will not become aware of the fact that new state has been filled downstream, so any updates that arrive through our VStream subscription will be discarded before they can reach the node with the newly filled state, so this state will become stale. This issue can be fixed (e.g. by adding “gaps” to all the upstream operators after receiving the result of the external upquery), but it has been implemented in its simplified form because that covers a large majority of currently known cases. The extended form can be implemented in the future if it turns out we have query patterns where this is necessary.

Overall, this new design to fill misses in the Boost cluster is wildly more efficient than the original design of Noria. The external upqueries performed in the middle of the dataflow allow us to fill a miss with performance that is _at worst_ equivalent to MySQL computing the query itself, and often much faster. The amount of data transmitted from Vitess to the Boost instance has been cut by orders of magnitude. However, the design introduces new complexity in the aforementioned data race, which we had to solve separately.

### Data races at any depth of the dataflow graph

The major change described in the previous section implies that upqueries (queries to the external Vitess cluster), which were previously performed solely on External Bases, can now be performed at any depth of the dataflow. The results of the original upqueries were always _replays_, because we were fetching ranges of individual rows from the underlying Base table. In these new upqueries performed on arbitrary SQL operators, the results are _not_ replays, because they contain the final results that need to be stored in the state tables, not the individual rows that must be processed by the operator. And yet these results are also vulnerable to the same data race between VStream changes and upquery rows which we’ve already discussed.

As we anticipated earlier, the way we initially fixed the data race was not fully correct. Let us understand the underlying issue in the previous version of the fix and see how a proper fix can also be applied to this more general problem:

The original fix, as described in the initial patent filing, keeps state in each of the External Bases of a Boost dataflow graph to ensure that rows that have been recently been seen in a VStream change event cannot be seen again in an in-flight upquery, and vice-versa. This check, which is performed essentially by comparing GTIDs, was performed _on the External Base_ itself. Incoming VStream changes were compared against the recent set of upqueries performed to the external base.

We can immediately intuit that this design will _not_ work after our new design for filling misses, because we no longer perform upqueries at the external base! The requests to the underlying Vitess cluster can be performed on any partially materialized operator throughout the dataflow graph, so filtering incoming change events cannot be performed in the External Base. It needs to be performed separately on each operator that can perform an external query. But simply porting our algorithm from the External Bases to all their children nodes is not enough to correctly fix the problem, as the underlying issue that applied to these bases is also relevant throughout when performing the algorithm on any other operator node.

The reason why this algorithm is not fully correct relates to the shape of the dataflow graph. Although in this document the graph of Bases, Readers and SQL operators has been discussed as a _tree_ (with a set of roots and leaves), in production environments it often looks more like a Directed Acyclic Graph (_DAG_), because individual nodes can be reused by more than one query.

The most obvious example of this are External Bases. If we have a Boost instance that is caching more than one query, and these queries depend on the same underlying SQL table, the dataflow will contain only _one_ External Base that represents the given SQL table, and this node will be shared by the dataflow of two or more queries. This generalizes to _any other SQL operator throughout the dataflow_. One of the important properties of Noria, which we’ve inherited in Boost, is that queries that share common patterns or subqueries can be efficiently cached by reusing parts of the dataflow, as to prevent the same partial state from being materialized more than once. This is a very powerful optimization in practice, but it complicates the algorithm to deduplicate rows between upqueries and VStream changes.

When analyzing the dataflow graph as a DAG, it becomes apparent that a VStream change event that arrives through a specific External Base must be fanned out to _all the children_ of this external base (which, as we’ve just explained, can be more than one because of node reuse), and for each one of those children, the change event must be fanned out again, and so on recursively. The event must be replicated throughout all the branches of the graph, because materialized state can exist on any SQL operator reachable from the External Base.

However, if we perform the same DAG analysis for the response of an external upquery (whether the result is a set of rows from an external base, or a final answer from an intermediate SQL operator), the resulting replay from the upquery will only be sent through the single path of the DAG that reaches the initial Reader that originally triggered a miss.

This dichotomy between the way VStream events are fully relayed through the DAG and upstream responses are relayed solely through _one path_ causes the original algorithm we designed to be incorrect. When a VStream event arrives through an External Base, we used to check if any of the upquery results that were recently received through the same base contained a row that was newer or equal to one of the rows in the VStream change event. If this was the case, we’d remove the row from the VStream change event, to prevent it from being seen twice. But this is clearly incorrect, because the resulting rows from the external upquery were only seen by _one of the paths_ of the DAG (the one that ends in the Reader that originally triggered the upquery), but the VStream change event needs to be seen _by all the paths_ of the DAG. Removing the row at the external base, like we were previously doing, would cause the data in _all but one_ of the paths of the DAG to become stale.

Taking this correctness issue into account, we can now amend the original design for row deduplication and generalize it as follows:

1. We define a GTID Tracker as an auxiliary data structure associated with any node in the dataflow graph that keeps partially materialized state. This applies to both External Bases and SQL operator nodes.
2. Every GTID Tracker tracks separately the recently seen upqueries for every path of the DAG that traverses the specific node. These paths are uniquely identified by a Tag, a strictly increasing identifier generated by our query planner.
3. When an upquery is triggered from a Reader, the upquery is marked with its corresponding Tag, again identifying the unique path through the DAG that the results of the upquery will traverse to arrive to the Reader.
4. If the upquery targets an External Base, or a partially materialized node directly reachable from an External base (this is, if the upquery will be resolved as an external upquery by reaching out to the underlying Vitess cluster), we store a record in the GTID tracker for the node. This record is global for the tracker (i.e. shared between all the separate replay paths that are tracked) and contains the _primary key_ that was requested for the external upquery.
5. When the upquery is finished, before we process its results:
	1. We access the Tag-scoped tracker inside the GTID Tracker for the node where the upquery was performed. We know the specific Tag because both the original upquery and its results are marked with a Tag, since the Tag represents the unique path through the DAG that the results of the upquery will traverse.
	2. For every record in the result of the external upquery, we store it in the set of recently seen upqueries for that Tag. We store the _primary key_ that uniquely identifies the record, and the GTID that the underlying database returned for the record. In our case, the underlying database is a Vitess cluster and the GTID is a MySQL [Global Transaction Identifier](https://dev.mysql.com/doc/refman/8.0/en/replication-gtids-concepts.html), but this can be generalized to any GTID-like value (i.e. a strictly-increasing counter or timestamp that allows us to compare whether a record comes _before_ or _after_ another record in time).
	3. We access the global set of in-flight queries for the GTID Tracker (this is _not_ Tag-scoped).
	4. We remove this upquery from the set of in-flight queries. If the in-flight record had any _buffered VStream change events_ accumulated in it (see item 6.3, below), we yield those events. They will be forwarded through the dataflow _right after_ the results of the upquery, so that the records in the events can be checked for recent duplicates.
	5. The result of the upquery is forwarded through the dataflow _unchanged_.
6. Meanwhile, when a VStream change event is processed by the GTID tracker for this node:
	1. We iterate through all the records in the change event. For every record, we check if any of the Tag-scoped trackers inside the GTID tracker has recently seen the record. We can efficiently check this by comparing the record’s _primary key_ with the _primary key_ we stored in the Tag-scoped tracker.
	2. For every record in the change event that has been recently seen by one of the Tag-scoped trackers, if the previously seen GTID is _newer our equal_ than the GTID for this new record, **we mark the record in the change event with the Tag where it was seen before**. Unlike in the original version of the algorithm, the record is **not** removed from the change event — it is simply tagged.
	3. Otherwise, if the record in the change event is a record that _could possibly be returned by one of the in-flight upqueries_ (we can check this because we know the _primary key_ for the in-flight upquery, and we know the primary key for this record): we buffer the full contents of the change event in the in-flight set for the GTID tracker. The buffered events will be yielded _after_ the in-flight query is finished, as explained in section 5.4. **Since the VStream change event has been buffered, we will skip processing it right now**. It is important to note that this whole check for a VStream change event will be applied _again_ after the buffered contents are yielded, and that this second check cannot trigger buffering again, because there won’t be any in-flight misses for the data we’ve already buffered.
7. Before we process _any record_ in any of the operators of the dataflow graph, we figure out all the Tags that belong to the operator’s node (this is, the set of all unique paths through the DAG that cross this specific node — this can be pre-computed at plan time). **If the record is tagged with any of the node’s Tags, we do not process it**.

This algorithm, which we believe to be novel, is applied on every node that contains a partial materialization, and is correct regardless of how many times this node is reused throughout the dataflow graph. It prevents any of the nodes from seeing duplicated packets between the external upqueries that are originated from each node and any VStream change events which the node observes. It does so by _tagging_ the records in VStream change events with unique identifiers for the paths in the DAG that have seen them before, and by delaying any of the VStream change events that could conflict with any existing in-flight external upqueries. This delay or buffering is pessimistic (it can delay events that won’t conflict in practice) but conservative (in practice, it delays a small subset of all the VStream change events that flow through the node).

### Partially Materialized Views for Range Queries

So far we have discussed Readers, the nodes where the output of the dataflow is partially materialized, as a hash table. This implementation allow us to fill misses for point queries (i.e. queries where the filter is an equality operator, such as `WHERE tbl.a = ?`) by storing the query parameter as the key to the hash table and three possible states as the value:

- The actual rows that are the result of the query, if they've been materialized
- An empty placeholder ("hole"), if the result of the query would return no rows when performed on the underlying data store
- Nothing (i.e. no entry in the hash table) if we don't know the result of the query because it has never been backfilled from the underlying data store.

This mechanism for partial materialization of point queries already exists in the original Noria. Here, we propose an extension to allow the dataflow engine to partially materialize **range** queries (this is, queries with the `>`, `>=`, `<` and `<=` operators).

Just like a traditional storage engine for a database, in order to serve range queries, the underlying data structure must be _ordered_. For this implementation in Boost, we have chosen a B-Tree, although any other alternative (binary tree, skip list, B+ tree, etc) would work equivalently. The ordered data structure allows us to serve a range query (e.g. `WHERE tbl.a >= ? AND tbl.a < ?`) by scanning all the entries in the tree between the two boundaries. The problem, exclusive to Boost and which we've solved in a novel way, is the partial materialization of rows and holes in the B-tree. When a query is performed on a Boost cluster and the partially materialized view must now be queried by a range (i.e. a lower bound, an upper bound, or both), how can we know if any of the data in the range is missing from the underlying data store, or whether it’s not missing but we’ve never fetched it yet?

For a point query, we solved the issue by performing an _upquery_ for the exact key that was missing, and storing in that key’s slot in the hash table either the resulting rows, or a hole if there were no results. For a range query, we would need to _upquery_ a whole range of values, but after storing them in the B-tree, how would subsequent user queries to non-equal but overlapping ranges know whether any data is missing or just hasn’t been fetched yet?

A possible solution to this issue is using a interval tree to track the ranges of the partially materialized view which have been filled. This is very accurate, although it has the drawback of propagating throughout the whole system: it’s not enough to use a interval tree to track seen ranges in the Reader. All the other operator nodes in the dataflow must similarly keep their own interval tree to track misses on their intermediate state. This adds a very significant amount of complexity to the system.

We propose a simpler design to work around this problem by introducing a constraint on the kind of range queries that can be partially materialized: they need to have _at least_ one equality operator in their filter conditions, besides an arbitrary amount of range operators. Hence, a query such as `WHERE tbl.a = ? AND tbl.b >= ? AND tbl.c > ?` could be partially materialized. A query such as `WHERE tbl.a > ? and tbl.b >= ?` could _not_ be partially materialized by this system, although we do support it in practice by performing full materialization, like a traditional materialization engine would.

By introducing this restriction, we scope down the complexity of keeping track of holes: we can return to a system very similar to the one we’re using for tracking holes for point queries by augmenting our ordered data store (a B-tree) with an auxiliary data structure that keeps tracks of which roots of the B-tree are _holes_. Here, roots means the prefix of the partial materialization key that applies to equality operators. With this approach, a query such as `WHERE tbl.a = ? AND tbl.b > ?` would only check whether `tbl.a = ?` has been filled before by looking up the prefix for `tbl.b` in the auxiliary data structure. If found, it means that the B-tree contains a comprehensive view of the underlying data store for all the values where `tbl.a = ?`, and hence can be queried with a range starting at `tbl.b > ?`. If missing, the _upquery_ we generate is scoped down to `tbl.a = ?`. Our goal is filling up a whole sub-tree of the partially materialized view, that will be processed by all the upstream dataflow operators without them needing to be aware of the fact that the view will be processing a whole range of values. From the operators’ point of view, they’re still processing a miss for a single point query.

This design was much simpler to implement compared to using interval trees throughout the dataflow, and provides very good real world performance characteristics. Even in pathological cases, such as where the equality filter in the query is for a column with very few unique values (causing the sub-tree that is being fetched to be very large), the system degrades into the equivalent of a fully materialized view. In common queries, which frequently have two or more equality operators as filter conditions, the amount of data fetched on each sub-tree is very similar to the theoretical minimum (i.e. what would be fetched if holes were being tracked by a interval tree).



### Arbitrary Filter Expressions

Lastly, we also propose a novel design for a dataflow engine that supports arbitrary filtering expressions with placeholders. Our existing design so far supports arbitrary filtering expressions as long as they don’t have placeholders. Any SQL filter expression such as `WHERE tbl.b NOT IN ('foo', 'bar', 'baz')` can be evaluated as part of the dataflow as long as the right-hand side of the expression is constant. When the right hand is a placeholder from the user’s query (e.g. `WHERE tbl.b = ?`), these expressions must be lifted outside of the dataflow: they act as the indexing key on which we query the partially materialized view. With our current design, our partially materialized view can be queried by any equality expression and by any range expression as long as it’s accompanied by at least one equality expression.

This novel design allows us to support _any_ SQL filter expression by paying a small performance penalty. To accomplish this, we present an algorithm for defining the indexing key of our partially materialized view that splits the indexing between **direct indexing** and **post-indexing**. This means that, given a set of arbitrary filter expressions with placeholders, we split the set between:

- The set of filter expressions that can be composed to create an indexing key into the partially materialized view
- The set of filter expressions that can only be applied as a post-processing step at query time

The logic of the algorithm is as follows: To construct the indexing key,

1. Append all the filter parameters with an equality (`=`) operator, regardless of order
2. Group all the _range_ parameters based on the column they’re filtering
3. Append the parameters for the largest group
4. Append all other parameters to the post-filtering step.

Let us see an example: For a complex query such as `WHERE tbl.a > ? AND tbl.b = ? and tbl.a <= ? AND tbl.c IN (?) AND tbl.d LIKE ?`, our algorithm selects the sub-set of filtering expressions that can be used to create an index into the partially materialized view. In this case, that would be `tbl.a > ? and tbl.b = ? AND tbl.a <= ?`, and the resulting index would be `[b, a]`, with queries ranging from `[b, a_lower]` to `[b, a_upper]` in an ordered data store.  The remaining filter operations, `tbl.c IN (?) AND tbl.d LIKE ?`, will be applied at query time.

With this approach, we split the dataflow in two phases: the traditional dataflow that we previously described, that results in a partially materialized view, and a query-time dataflow step that is performed on the sub-set of results that are returned from the partially materialized view. By creating a broader materialized view, we have a slightly less efficient cache that supports any arbitrary filtering expression by paying a performance cost of delaying part of the dataflow operators at query time.

The query-time dataflow step is defined as a set of operations performed at query time based on the output of a broader partially materialized result set and the parameters the user has supplied. These operations are implemented using the same primitives as the dataflow, to ensure results are consistent, and they have two phases: **filtering** and **aggregation**.

The filtering phase is applied to all queries with post-indexing. It filters out all the rows that have been fetched from the materialized view and that do not match the post filters, by evaluating the filtering expressions using the user’s placeholders. For instance, for a query such as `SELECT tbl.* FROM tbl WHERE tbl.a = ? AND tbl.b NOT IN (?)`, the result of the partially materialized view are all the rows where `tbl.a` equals the user’s first placeholder, and this result is further filtered down by applying the `tbl.b NOT IN(?)` expression with the user’s second placeholder.

The aggregation phase is applied to those queries that have post-indexing and whose select statement contains aggregated functions (e.g. `COUNT`, `SUM`, `MIN`, `MAX`, etc). This is required because these queries must materialized into result sets that are broader than the user expects (as we’ve seen in the previous filtering example), and they also have a higher cardinality, to take into account the values for the filter expressions that were not part of the partial materialization key. This is a novel and important part of the post-filtering implementation, because it requires changes on the dataflow for the query.

Let us see an example. For an aggregation query such as `SELECT COUNT(*) FROM tbl WHERE tbl.a = ? AND tbl.b NOT IN (?)`, where one of the filter operators cannot be used as a key into the partially materialized view, the trivial materialization of the query would **not** work for applying a post processing step. If we simply use `SELECT COUNT(*) WHERE tbl.a = ?` as the broader query that will be materialized, there is no way to apply `WHERE tbl.b NOT IN (?)` at query time, because the values for `tbl.b` have been collapsed into the aggregation. To support these cases, our dataflow query planner must be extended to _push down_ all the data dependencies for post-filtering operators all the way to the view. Hence, the broader query that is partially materialized here would be `SELECT COUNT(*), tbl.b from tbl WHERE tbl.a = ? GROUP BY tbl.b`. The result set for the `a` placeholder can now be filtered on `tbl.b NOT IN (?)` in the post-indexing filtering phase previously described, because `tbl.b` is now part of the returned rows. However, simply applying the filtering operation on the returned rows at query time is not sufficient to generate the correct result. The output of a `COUNT(*)` operation must be a single row, and we have a range of rows, grouped by `b`, that must be aggregated again.

This aggregation operator is implemented using the same primitives as the aggregation nodes in the dataflow, and performs the correct supra-aggregation for each operator kind: `COUNT`s are _summed_ (not counted again), `SUM`s are summed again, and `MAX` and `MIN` are re-applied to find the maximum/minimum out of all the partial maximums/minimums.

### Conclusion

Although the first beta of Boost which we announced was very closely based on the original design of Noria, this new architecture is a significant divergence in design that exploits the functionality the underlying Vitess cluster and MySQL instances to move a lot of expensive computation away from the Boost cache. Performing external upqueries at any depth of the dataflow graph and resolving duplication in any of the nodes that can perform an external upquery are novel algorithms which are not covered or hinted at in the original Noria paper or PhD dissertation. With the addition of partially materialized range queries and post-filtering operators, we believe the new architecture supports a significantly wider array of SQL patterns than the original Noria without having any pathological performance cases during misses, and without suffering any correctness issues due to duplicated data.
